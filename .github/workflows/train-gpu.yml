name: Train on GPU

on:
  workflow_dispatch:
    inputs:
      ckpt:
        description: "Resume from checkpoint (optional, e.g. checkpoints/step_2000.pt)"
        required: false
        default: ""

jobs:
  train:
    # TODO: replace with your org's GPU runner label
    runs-on: ubuntu-22.04-gpu-t4
    timeout-minutes: 4320  # up to 3 days; adjust for your run

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"   # stable for CUDA wheels
      # Optional: CUDA toolkit (many GPU runners already have it)
      # - name: Install CUDA Toolkit
      #   uses: Jimver/cuda-toolkit@v0.2.19
      #   with: { cuda: "12.4.1" }

      - name: Install deps (GPU wheels)
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip uninstall -y torch torchvision torchaudio || true
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

      - name: Print CUDA info
        run: |
          python - << 'PY'
          import torch
          print('cuda_available:', torch.cuda.is_available())
          print('torch.cuda:', torch.version.cuda)
          print('device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'â€”')
          PY

      - name: Prepare cache dirs
        run: |
          mkdir -p checkpoints
          echo "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128" >> $GITHUB_ENV

      - name: Train
        env:
          CKPT: ${{ github.event.inputs.ckpt }}
        run: |
          set -e
          python -V
          if [ -n "$CKPT" ]; then
            python train.py --config configs/recommended.yaml --ckpt "$CKPT"
          else
            python train.py --config configs/recommended.yaml
          fi

      - name: Upload checkpoints
        uses: actions/upload-artifact@v4
        with:
          name: checkpoints
          path: checkpoints/
          retention-days: 14
